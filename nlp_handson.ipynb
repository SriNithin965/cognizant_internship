{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_handson.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNS8ZX+K/R2V6nEqYyNBhRN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SriNithin965/cognizant_internship/blob/main/nlp_handson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSTALLATION**"
      ],
      "metadata": {
        "id": "NajIgqmksr1b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgo0m01EUSD0"
      },
      "outputs": [],
      "source": [
        "!pip install textacy\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOADING THE DATASET AND IMPORTING PACKAGES**"
      ],
      "metadata": {
        "id": "znAybjbHswpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import textacy\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "GjvzxzKfVZGM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib3 # the lib that handles the url stuff\n",
        "path = 'https://raw.githubusercontent.com/SriNithin965/cognizant_internship/main/text_nlp.txt'\n",
        "http = urllib3.PoolManager()\n",
        "r = http.request('GET', path)\n",
        "data = r.data.decode('utf-8')\n",
        "data = str(data)"
      ],
      "metadata": {
        "id": "iSQaJB4m1jkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SENTENCES SEGMENTATION**"
      ],
      "metadata": {
        "id": "GXbwtoy_YRzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sen = nlp(data).sents\n",
        "data_sens = []\n",
        "for line in sen:\n",
        "  data_sens.append(str(line))\n",
        "data_sens"
      ],
      "metadata": {
        "id": "UAZfmG1A2JgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Tokenization**"
      ],
      "metadata": {
        "id": "r11wqWdOb06d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_word = []\n",
        "for line in data_sens:\n",
        "  word = nlp(line)\n",
        "  for token in word:\n",
        "    data_word.append(token.text)\n",
        "data_word "
      ],
      "metadata": {
        "id": "JCuN7zBCbz1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PARTS OF SPEECH**"
      ],
      "metadata": {
        "id": "FiD94xNLgysA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_part = {}\n",
        "for line in data_sens:\n",
        "  word = nlp(line)\n",
        "  for token in word:\n",
        "    data_part[token.text] = token.pos_\n",
        "data_part"
      ],
      "metadata": {
        "id": "xXSpSwJMduyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LEMMATATION**"
      ],
      "metadata": {
        "id": "f3nd3hmg6dSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_lemma = {}\n",
        "for line in data_sens:\n",
        "  word = nlp(line)\n",
        "  for token in word:\n",
        "    data_lemma[token.text] = token.lemma_\n",
        "data_lemma"
      ],
      "metadata": {
        "id": "-y5eJeyO6grI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REMOVING STOP WORDS**"
      ],
      "metadata": {
        "id": "8RRXNznF7NQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "filtered_sentence = [w for w in data_word if not w.lower() in stop_words]\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in data_word:\n",
        "\tif w not in stop_words:\n",
        "\t\tfiltered_sentence.append(w)\n",
        "\n",
        "filtered_sentence\n"
      ],
      "metadata": {
        "id": "zzE4AWkG7TxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DEPENDENCY PARSING**"
      ],
      "metadata": {
        "id": "rmMwbLR7z1zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dependancy = []\n",
        "doc = nlp(data)\n",
        "for token in doc:\n",
        "    data_dependancy.append((token.text, token.dep_, token.head.text,token.pos_,\n",
        "            [child for child in token.children]))\n",
        "data_dependancy"
      ],
      "metadata": {
        "id": "yEilNExUz_Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VERB PHRASES**"
      ],
      "metadata": {
        "id": "Ml_7cEIE00Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "patterns = [{\"POS\":\"NOUN\"},{\"POS\":\"VERB\"}]\n",
        "matcher.add(\"VERB PHRASES\", [patterns])\n",
        "data_verb_phrases =[]\n",
        "doc = nlp(data)\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    string_id = nlp.vocab.strings[match_id] \n",
        "    span = doc[start:end]\n",
        "    data_verb_phrases.append(span.text)\n",
        "data_verb_phrases"
      ],
      "metadata": {
        "id": "r90hxfHrFGqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NAMED ENTITIES**"
      ],
      "metadata": {
        "id": "PAKkKQ8xHMtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(data)\n",
        "for ent in doc2.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "QiZGSz73HQtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Morphology**"
      ],
      "metadata": {
        "id": "xHJ2jBWIpfeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_morph = []\n",
        "doc = nlp(data)\n",
        "for text in doc:\n",
        "  data_morph.append(text.morph)"
      ],
      "metadata": {
        "id": "3hnJr9t2ppJq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**noun chunks**"
      ],
      "metadata": {
        "id": "1Va0EnbPrRDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(data)\n",
        "noun_chunks = textacy.extract.noun_chunks(doc, min_freq=3)\n",
        "data_chucks = []\n",
        "noun_chunks = map(str, noun_chunks)\n",
        "noun_chunks = map(str.lower, noun_chunks)\n",
        "\n",
        "for noun_chunk in set(noun_chunks):\n",
        "    if len(noun_chunk.split(\" \")) > 1:\n",
        "        data_chucks.append(noun_chunk)\n",
        "\n",
        "data_chucks"
      ],
      "metadata": {
        "id": "iu7rBUCKrHTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**acronyms**"
      ],
      "metadata": {
        "id": "mZtwH9fyr9bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(data)\n",
        "data_arc = []\n",
        "acr = textacy.extract.acronyms(doc)\n",
        "for ac in acr:\n",
        "  data_arc.append(ac)\n",
        "\n",
        "data_arc"
      ],
      "metadata": {
        "id": "urkjXpQ7rvHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ngrams**"
      ],
      "metadata": {
        "id": "k60vMiyxsYD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(data)\n",
        "data_ngrams = []\n",
        "acr = textacy.extract.ngrams(doc,3)\n",
        "for ac in acr:\n",
        "  data_ngrams.append(ac)\n",
        "\n",
        "data_ngrams"
      ],
      "metadata": {
        "id": "s6xCEQw1sL19"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}